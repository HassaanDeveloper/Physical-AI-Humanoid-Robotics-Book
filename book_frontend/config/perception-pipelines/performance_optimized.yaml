# Performance Optimized Perception Pipeline
# This configuration maximizes performance for real-time applications
# with aggressive GPU acceleration and memory optimization

# Performance Configuration
performance:
  name: "performance_optimized"
  description: "High-performance perception pipeline with GPU acceleration"
  version: "1.3.0"

  # GPU Configuration
  gpu:
    enabled: true
    device_id: 0
    memory_limit: 3072  # MB

    optimization:
      enable_tensor_cores: true
      enable_fp16: true
      enable_tensorrt: true
      max_workspace_size: 1024  # MB

    memory:
      enable_pooling: true
      pool_size: 1024  # MB
      max_cache_size: 512  # MB

  # Pipeline Settings
  pipeline:
    max_fps: 60
    target_latency: 0.016  # seconds (60 FPS)
    batch_size: 4

    # Processing stages
    stages:
      - name: "preprocessing"
        gpu_accelerated: true
        max_threads: 2

      - name: "inference"
        gpu_accelerated: true
        max_threads: 4

      - name: "postprocessing"
        gpu_accelerated: false
        max_threads: 2

  # Model Configuration
  model:
    type: "yolov4-tiny"
    path: "/models/yolov4-tiny.onnx"
    engine_path: "/models/yolov4-tiny.engine"

    optimization:
      precision: "fp16"
      max_batch_size: 8
      workspace_size: 512  # MB

    caching:
      enable_engine_caching: true
      cache_path: "/tmp/model_cache"

  # Image Processing
  image_processing:
    resize:
      enabled: true
      width: 416
      height: 416
      method: "gpu"  # GPU-accelerated resize

    normalize:
      enabled: true
      method: "gpu"  # GPU-accelerated normalization

    color_space: "bgr"  # Optimized for YOLO

  # Inference Configuration
  inference:
    confidence_threshold: 0.4
    nms_threshold: 0.45
    max_detections: 200

    optimization:
      use_cuda_graphs: true
      enable_timing_cache: true
      prefer_pinned_memory: true

  # Memory Management
  memory:
    pinned_memory:
      enabled: true
      pool_size: 256  # MB

    zero_copy:
      enabled: true
      max_transfers: 8

  # Multi-threading
  threading:
    enable_parallel_pipeline: true
    enable_async_transfers: true
    thread_priority: "high"

    pools:
      - name: "preprocessing"
        size: 2
        priority: "normal"

      - name: "inference"
        size: 4
        priority: "high"

      - name: "postprocessing"
        size: 2
        priority: "normal"

  # Performance Monitoring
  monitoring:
    enabled: true
    interval: 1  # frame

    metrics:
      - "gpu_utilization"
      - "gpu_memory"
      - "cpu_utilization"
      - "latency"
      - "throughput"

    alerts:
      high_latency: 0.033  # 30 FPS threshold
      high_gpu_memory: 2800  # MB
      low_throughput: 20  # detections/second

  # Adaptive Performance
  adaptive:
    enabled: true
    update_interval: 10  # frames

    strategies:
      - name: "reduce_resolution"
        trigger: "high_latency"
        condition: "latency > 0.033"
        action: "reduce_resolution"
        params: {scale: 0.9}

      - name: "reduce_batch_size"
        trigger: "high_memory"
        condition: "gpu_memory > 2800"
        action: "reduce_batch_size"
        params: {decrement: 1}

      - name: "increase_batch_size"
        trigger: "low_latency"
        condition: "latency < 0.016"
        action: "increase_batch_size"
        params: {increment: 1, max: 8}

  # Power Management
  power:
    mode: "max_performance"
    gpu_clock: "max"
    memory_clock: "max"

# ROS 2 Integration
ros2:
  node_name: "performance_perception"
  namespace: "perception"

  # Quality of Service (optimized for performance)
  qos:
    reliability: "best_effort"
    durability: "volatile"
    history_depth: 5
    depth: 1

  # Transport Configuration
  transport:
    use_intra_process: true
    use_shared_memory: true
    max_message_size: 8192  # bytes

# Input/Output Configuration
io:
  input:
    image_topic: "/camera/color/image_raw"
    camera_info_topic: "/camera/color/camera_info"
    queue_size: 3

  output:
    detections_topic: "/perception/detections"
    debug_topic: "/perception/debug"
    performance_topic: "/perception/performance"

    # Zero-copy transport
    zero_copy:
      enabled: true
      max_messages: 10

# Debug and Logging
debug:
  enabled: false
  visualization: false

  logging:
    level: "warn"
    performance_only: true
    interval: 10  # frames

# Safety Limits
safety:
  max_gpu_memory: 3072  # MB
  max_cpu_usage: 90  # percent
  max_latency: 0.05  # seconds

  actions:
    on_gpu_limit: "reduce_quality"
    on_cpu_limit: "reduce_threads"
    on_latency_limit: "skip_frames"

# Environment Configuration
environment:
  CUDA_VISIBLE_DEVICES: "0"
  NITROS_LOG_LEVEL: "warn"
  TF_CPP_MIN_LOG_LEVEL: "2"

# Health Monitoring
health:
  check_interval: 0.1  # seconds
  response_time: 0.05  # seconds

  checks:
    gpu_health: true
    memory_health: true
    latency_health: true
    throughput_health: true

# Error Recovery
recovery:
  strategies:
    - name: "gpu_memory_exceeded"
      condition: "gpu_memory > 3000"
      action: "restart_node"
      delay: 5.0

    - name: "high_latency_persistent"
      condition: "latency > 0.1 for 10 frames"
      action: "reduce_quality"
      params: {scale: 0.7}

# Validation Rules
validation:
  input:
    min_width: 320
    max_width: 1920
    min_height: 240
    max_height: 1080

  output:
    min_confidence: 0.3
    max_detections: 500

# Metrics Collection
metrics:
  collection_interval: 0.1  # seconds
  window_size: 100  # frames

  topics:
    - "/perception/performance_metrics"
    - "/statistics"

  recorded:
    - "fps"
    - "latency"
    - "gpu_utilization"
    - "gpu_memory"
    - "cpu_utilization"
    - "detection_count"
    - "inference_time"
    - "preprocessing_time"
    - "postprocessing_time"